id: crime_schedule
namespace: phillycrime
description: |
  Flow to grab philadelphia crime data from open data philly website every day. 

inputs:
    # Input to specify the year
  - id: year
    type: SELECT
    displayName: Select year
    values: ['2006', '2007', '2008', '2009' , '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025']
    defaults: "2025"
  
variables:
  base_url_post: 'https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%27'  
  base_url_pre: 'https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&skipfields=cartodb_id,the_geom,the_geom_webmercator&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%27'
  mid_url: '-01-01%27%20AND%20dispatch_date_time%20%3C%20%27'
  end_url: '-01-01%27'
  full_url_post: "{{vars.base_url_post}}{{inputs.year}}{{vars.mid_url}}{{inputs.year | number + 1}}{{vars.end_url}}"
  full_url_pre: "{{vars.base_url_pre}}{{inputs.year}}{{vars.mid_url}}{{inputs.year | number + 1}}{{vars.end_url}}"
  output_name: "crime_{{inputs.year}}.csv"
  gcs_file: "gs://{{kv('GCP_BUCKET_NAME')}}/{{vars.output_name}}"
  table: "{{kv('GCP_DATASET')}}.Crime_{{inputs.year}}"
  data_post: "{{outputs.extractpost.outputFiles['crime_'~inputs.year~'.csv']}}"
  data_pre: "{{outputs.extractpre.outputFiles['crime_'~inputs.year~'.csv']}}"

tasks:
  - id: set_label
    type: io.kestra.plugin.core.execution.Labels
    labels:
      file: "{{render(vars.output_name)}}"

    # Check if >= 2020
  - id: if_post_2020
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.year | number >= 2020}}"
    then:
    # Task to download the crime csv file 
    - id: extractpost
      type: io.kestra.plugin.scripts.shell.Commands
      outputFiles:
        - "*.csv"
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      commands:
        - curl -s -o {{render(vars.output_name)}} -L '{{render(vars.full_url_post)}}'

    - id: transform_data_post
      type: io.kestra.plugin.scripts.python.Script
      outputFiles:
        - '{{render(vars.output_name)}}'
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      warningOnStdErr: false
      beforeCommands:
      - pip install pandas geopy
      script: | 
        import pandas as pd
        import requests
        from datetime import date, timedelta

        
        # Load csv
        df = pd.read_csv("{{render(vars.data_post)}}")
        
        # Clean a little
        keep_cols = ['objectid', 'dc_dist', 'dispatch_date', 'hour', 'location_block', 'text_general_code', 'ucr_general', 'lat', 'lng']
        df = df[keep_cols]
        df['hour'] = df['hour'].fillna(0)
        df['dispatch_date'] = pd.to_datetime(df['dispatch_date'])
        df['hour'] = df['hour'].astype(int)
        
        # Only get past weeks worth as to not have to find zip for everything again
        df[pd.to_datetime(df['dispatch_date']) >= pd.to_datetime(date.today() - timedelta(days=7))]

        def get_zip(lat, lon):
          url = f'https://nominatim.openstreetmap.org//reverse?lat={lat}&lon={lon}&format=json&addressdetails=1'
          try:
            result = requests.get(url=url,headers = {'User-Agent':'me'})
            result_json = result.json()
            return result_json['address']['postcode']
          except:
            return None  

        df['zip'] = df.apply(lambda row: get_zip(row['lat'], row['lng']), axis=1)
        df['zip'] = df['zip'].astype('Int64')

        df.to_csv('{{render(vars.output_name)}}', index=False)

    - id: upload_gcs_post
      type: io.kestra.plugin.gcp.gcs.Upload
      from: "{{outputs.transform_data_post.outputFiles[render(vars.output_name)]}}"
      to: "{{render(vars.gcs_file)}}"


  - id: if_pre_2020
    type: io.kestra.plugin.core.flow.If
    condition: "{{inputs.year | number < 2020}}"
    then:
    # Task to download the crime csv file 
    - id: extractpre
      type: io.kestra.plugin.scripts.shell.Commands
      outputFiles:
        - "*.csv"
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      commands:
        - curl -o {{render(vars.output_name)}} -L '{{render(vars.full_url_pre)}}' 

    - id: transform_data_pre
      type: io.kestra.plugin.scripts.python.Script
      outputFiles:
        - '{{render(vars.output_name)}}'
      taskRunner:
        type: io.kestra.plugin.core.runner.Process
      warningOnStdErr: false
      beforeCommands:
      - pip install pandas geopy
      script: | 
        import pandas as pd
        import requests
        from datetime import date, timedelta
        
        # Load csv
        df = pd.read_csv("{{render(vars.data_pre)}}")
        
        # Clean a little
        keep_cols = ['objectid', 'dc_dist', 'dispatch_date', 'hour', 'location_block', 'text_general_code', 'ucr_general', 'lat', 'lng']
        df = df[keep_cols]
        df['hour'] = df['hour'].fillna(0)
        df['dispatch_date'] = pd.to_datetime(df['dispatch_date'])
        df['hour'] = df['hour'].astype(int)

        def get_zip(lat, lon):
          url = f'https://nominatim.openstreetmap.org//reverse?lat={lat}&lon={lon}&format=json&addressdetails=1'
          try:
            result = requests.get(url=url,headers = {'User-Agent':'me'})
            result_json = result.json()
            return result_json['address']['postcode']
          except:
            return None  

        df['zip'] = df.apply(lambda row: get_zip(row['lat'], row['lng']), axis=1)
        df['zip'] = df['zip'].astype('Int64')

        df.to_csv('{{render(vars.output_name)}}', index=False)

    - id: upload_gcs_pre
      type: io.kestra.plugin.gcp.gcs.Upload
      from: "{{outputs.transform_data_pre.outputFiles[render(vars.output_name)]}}"
      to: "{{render(vars.gcs_file)}}"           

   # Create the BQ table that will hold final data
  - id: bq_crimedata
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
          CREATE TABLE IF NOT EXISTS {{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.phillycrimes
          (
            unique_row_id BYTES OPTIONS(description='A unique identifier for the trip, generated by hashing key trip attributes.')
            ,filename STRING OPTIONS(description='Source filename')
            ,objectid INTEGER OPTIONS(description='Unique identifier for row from the source')
            ,dc_dist INTEGER OPTIONS(description='Integer representing the police district where the crime happened')
            ,dispatch_date DATE OPTIONS(description='Date the crime happened')
            ,hour INTEGER OPTIONS(description='Hour the crime happened')
            ,location_block STRING OPTIONS(description='String representing the block the crime happened on')
            ,text_general_code STRING OPTIONS(description='String representing category of crime committed')
            ,ucr_general INTEGER OPTIONS(description='The rounded crime code')
            ,lat BIGNUMERIC OPTIONS(description='Represents the latitude coordinate of crime')
            ,lng BIGNUMERIC OPTIONS(description='Represents the longitude coordinate of crime')
            ,zip INTEGER OPTIONS(description='Zipcode the crime happened in')
          )
          PARTITION BY dispatch_date;

    # External table to allow you to query data outside of BQ, but without having to load it into BQ storage
  - id: bq_crimedata_ext
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
          CREATE OR REPLACE EXTERNAL TABLE {{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext
          (
            objectid INTEGER OPTIONS(description='Unique identifier for row from the source')
            ,dc_dist INTEGER OPTIONS(description='Integer representing the police district where the crime happened')
            ,dispatch_date DATE OPTIONS(description='Date the crime happened')
            ,hour INTEGER OPTIONS(description='Hour the crime happened')
            ,location_block STRING OPTIONS(description='String representing the block the crime happened on')
            ,text_general_code STRING OPTIONS(description='String representing category of crime committed')
            ,ucr_general INTEGER OPTIONS(description='The rounded crime code')
            ,lat BIGNUMERIC OPTIONS(description='Represents the latitude coordinate of crime')
            ,lng BIGNUMERIC OPTIONS(description='Represents the longitude coordinate of crime')
            ,zip INTEGER OPTIONS(description='Zipcode the crime happened in')
          )
          OPTIONS (
              format = 'CSV',
              uris = ['{{render(vars.gcs_file)}}'],
              skip_leading_rows = 1,
              ignore_unknown_values = TRUE
          );       

    # Temp table to create new columns. Will be used to merge to final
  - id: bq_crimedata_tmp
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
          CREATE OR REPLACE TABLE {{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}
          AS
          SELECT
            MD5(CONCAT(
              COALESCE(CAST(objectid AS STRING), ""),
              COALESCE(CAST(dc_dist AS STRING), ""),
              COALESCE(CAST(dispatch_date AS STRING), "")
            )) AS unique_row_id,
            '{{render(vars.output_name)}}' AS filename,
            *
          FROM {{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext;

  - id: bq_crime_merge
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
          MERGE INTO {{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.phillycrimes T
          USING {{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}} S
          ON T.unique_row_id = S.unique_row_id
          WHEN NOT MATCHED THEN
            INSERT (unique_row_id, filename, objectid, dc_dist, dispatch_date, hour, location_block, text_general_code, ucr_general, lat, lng, zip)
            VALUES (S.unique_row_id, S.filename, S.objectid, S.dc_dist, S.dispatch_date, S.hour, S.location_block, S.text_general_code, S.ucr_general, S.lat, S.lng, S.zip);

  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles

  # Delete temp tabls
  - id: cleanup_temp_tables
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      -- Delete temporary yearly table
      DROP TABLE IF EXISTS {{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}};

      -- Delete external table
      DROP EXTERNAL TABLE IF EXISTS {{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext;

    # Delete bucket after use 
  - id: delete
    type: io.kestra.plugin.gcp.gcs.Delete
    uri: gs://{{kv('GCP_BUCKET_NAME')}}/{{render(vars.output_name)}}

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      serviceAccount: "{{kv('GCP_CREDS')}}"
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"

# Run at 1pm everyday
triggers:
  - id: crime_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 13 * * *"
    timezone: America/New_York
    inputs:
      year: 2025
      

